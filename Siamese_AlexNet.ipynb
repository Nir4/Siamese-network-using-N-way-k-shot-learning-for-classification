{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-8k2fnQY1PPR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, datasets, models\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import random\n",
        "from torchvision import transforms\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLd98jrc1Uzb",
        "outputId": "5fa9daf0-a7a1-4e27-ad8c-5f4c2ea35ee3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining Similarity Metrics"
      ],
      "metadata": {
        "id": "W524pmNvPMFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class PearsonCorrelationLoss(nn.Module):\n",
        "    def _init_(self):\n",
        "        super(PearsonCorrelationLoss, self)._init_()\n",
        "\n",
        "    def forward(self, predicted, target):\n",
        "        predicted_mean = torch.mean(predicted)\n",
        "        target_mean = torch.mean(target)\n",
        "\n",
        "        covar = torch.mean((predicted - predicted_mean) * (target - target_mean))\n",
        "        predicted_var = torch.mean((predicted - predicted_mean)**2)\n",
        "        target_var = torch.mean((target - target_mean)**2)\n",
        "\n",
        "        # Calculate the Pearson correlation coefficient\n",
        "        pearson_corr = covar / (torch.sqrt(predicted_var) * torch.sqrt(target_var))\n",
        "\n",
        "        # The loss is the negative of the correlation coefficient to be minimized\n",
        "        loss = -pearson_corr\n",
        "\n",
        "        return loss\n",
        "\n",
        "pearson_correlation = PearsonCorrelationLoss()\n",
        "\n",
        "class CustomSigmoid(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomSigmoid, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Implement the sigmoid function using the torch.sigmoid function\n",
        "        sigmoid_x = torch.sigmoid(x)\n",
        "        return sigmoid_x\n",
        "\n",
        "# Example usage:\n",
        "custom_sigmoid = CustomSigmoid()\n",
        "\n",
        "class CustomCosineSimilarity(nn.Module):\n",
        "    def __init__(self, eps=1e-8):\n",
        "        super(CustomCosineSimilarity, self).__init__()\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        # Compute the cosine similarity between x1 and x2\n",
        "        dot_product = torch.sum(x1 * x2, dim=-1)\n",
        "        norm_x1 = torch.norm(x1, dim=-1, p=2) + self.eps\n",
        "        norm_x2 = torch.norm(x2, dim=-1, p=2) + self.eps\n",
        "        cosine_sim = dot_product / (norm_x1 * norm_x2)\n",
        "        return cosine_sim\n",
        "\n",
        "# Example usage:\n",
        "\n",
        "custom_cosine_similarity = CustomCosineSimilarity()\n",
        "\n",
        "# similarity = custom_cosine_similarity(x1, x2)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9Dvaq-Wu1gE_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tweaking the no. of shots"
      ],
      "metadata": {
        "id": "oLl60P39PXbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shot=14"
      ],
      "metadata": {
        "id": "xiZOtlHGJrCL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing the data for suitable input shape"
      ],
      "metadata": {
        "id": "OoU6YCPEPYaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import itertools\n",
        "\n",
        "def change_size(dataset_root, dataset_root_new):\n",
        "    class_folders = os.listdir(dataset_root)\n",
        "    for class_folder in class_folders:\n",
        "        wherewe_have_to_add = os.path.join(dataset_root_new, class_folder)\n",
        "        class_path = os.path.join(dataset_root, class_folder)\n",
        "        os.makedirs(wherewe_have_to_add)\n",
        "        image_files = os.listdir(class_path)\n",
        "        for i in image_files:\n",
        "            new_path = os.path.join(class_path, i);\n",
        "            grayscale_image = Image.open(new_path)\n",
        "\n",
        "            # Create a new RGB image with the desired size\n",
        "            new_image = Image.new(\"RGB\", (224, 224), \"white\")\n",
        "\n",
        "            # Paste the grayscale image onto the RGB image's center\n",
        "            x_offset = (224 - 105) // 2\n",
        "            y_offset = (224 - 105) // 2\n",
        "            new_image.paste(grayscale_image, (x_offset, y_offset))\n",
        "\n",
        "            # Convert the image to a NumPy array\n",
        "            rgb_image = np.array(new_image)\n",
        "\n",
        "            # Create three channels by copying the single-channel data to all channels\n",
        "            rgb_image = np.stack([rgb_image] * 3, axis=-1)\n",
        "\n",
        "            # Save the modified RGB image to a specific directory with a new name\n",
        "            new_image.save(os.path.join(wherewe_have_to_add, i))\n",
        "\n",
        "            # Display the RGB image\n",
        "            new_image.show()\n",
        "\n",
        "\n",
        "\n",
        "def generate_test_representative(dataset_root, no_of_shot):\n",
        "    images = []\n",
        "    class_folders = os.listdir(dataset_root)\n",
        "    #Test Data\n",
        "    for class_ in class_folders:\n",
        "      path_class = os.path.join(dataset_root,class_ )\n",
        "      image_in_class = os.listdir(path_class)\n",
        "      for image in image_in_class:\n",
        "        images.append(os.path.join(path_class, image))\n",
        "    #random.shuffle(images)\n",
        "\n",
        "    #Representative data\n",
        "    rep_data = []\n",
        "    for class_ in class_folders:\n",
        "      path_class = os.path.join(dataset_root,class_ )\n",
        "      images_in_class = os.listdir(path_class)\n",
        "      # print(images_in_class)\n",
        "      random_values = random.sample(images_in_class, no_of_shot)\n",
        "      # print(random_values)\n",
        "      to_add = []\n",
        "      for basename in random_values:\n",
        "        to_add.append(os.path.join(path_class, basename))\n",
        "      rep_data.append(to_add)\n",
        "    return (images, rep_data)\n",
        "\n",
        "\n",
        "dataset_root     = \"/content/drive/My Drive/Greek\"\n",
        "dataset_root_new = \"/content/drive/My Drive/Greek_new\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HtB4LdWE1gs0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(images, rep_data) = generate_test_representative(dataset_root_new, shot )\n",
        "images_path = images\n",
        "rep_data_path = rep_data\n"
      ],
      "metadata": {
        "id": "jlrkCYrJ1oCl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(images_path[0:2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SSwMUwV1qMd",
        "outputId": "c3126239-a7c3-47a6-edb0-5e882b497494"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/drive/My Drive/Greek_new/character03/0396_20.png', '/content/drive/My Drive/Greek_new/character03/0396_09.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for i in range(0, len(images)):\n",
        "      image  = Image.open(images[i])\n",
        "      images[i] = transforms.ToTensor()(image)\n",
        "      #imgplot = plt.imshow(img)\n",
        "      images[i] = torch.tensor(images[i])\n",
        "print(images[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f77ukQUQ1uIM",
        "outputId": "eb4e614b-b7be-4d37-f9aa-978c1c6f930e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-38075824437d>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  images[i] = torch.tensor(images[i])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 224, 224])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "for i in range(0, len(rep_data)):\n",
        "    for j in range(0,len(rep_data[0])):\n",
        "      image =Image.open(rep_data[i][j])\n",
        "      #imgplot  = plt.imshow(img)\n",
        "      rep_data[i][j]= transforms.ToTensor()(image)\n",
        "print(rep_data[0][0].shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jbp6_XUx1w7D",
        "outputId": "ac4c47d2-9f39-4b54-91c1-461026a9b7d2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 224, 224])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating pretrained model"
      ],
      "metadata": {
        "id": "K372fQ24Pg7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "def general_model(index, images,rep_images ):\n",
        "\n",
        "  # Extract the last layer (before classification)\n",
        "  alexnet_model = models.alexnet(pretrained=True)\n",
        "\n",
        "  # Extract the feature layers (excluding the classification layers)\n",
        "  pretrained_features = nn.Sequential(*list(alexnet_model.features.children())[:-1])\n",
        "\n",
        "  Final = []\n",
        "  for image_sample in images:\n",
        "    print(image_sample.shape)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      image_tensor = image_sample.unsqueeze(0)  # Add a batch dimension\n",
        "      features1 = alexnet_model(image_tensor)\n",
        "    fin_output = []\n",
        "    for class_ in rep_images:\n",
        "      prob = []\n",
        "      for image in class_:\n",
        "        with torch.no_grad():\n",
        "          image_tensor = image.unsqueeze(0)  # Add a batch dimension\n",
        "          features2 = alexnet_model(image_tensor)\n",
        "\n",
        "        similarity_index = index(features1, features2)\n",
        "        prob.append(similarity_index)\n",
        "      val = 0;\n",
        "      for value in prob:\n",
        "        val+=value\n",
        "\n",
        "      fin_output.append(val/len(prob))\n",
        "    ind = 0\n",
        "    maxi = 0\n",
        "    for i in range(len(fin_output)-1):\n",
        "      if(fin_output[i] > maxi):\n",
        "        maxi = max( maxi, fin_output[i])\n",
        "        ind = i\n",
        "    Final.append([ind, maxi])\n",
        "\n",
        "\n",
        "\n",
        "  return Final\n",
        "# You can use `features1` and `features2` for similarity calculation (e.g., cosine similarity) or any other task you have in mind.\n"
      ],
      "metadata": {
        "id": "3B5PsSA71xkx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dinal=general_model(custom_cosine_similarity,images[0:20],rep_data)\n",
        "print(dinal)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZ5918e010t8",
        "outputId": "a2e1eb7b-0764-43d5-bbdf-ed34d3373684"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n",
            "100%|██████████| 233M/233M [00:01<00:00, 123MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 224, 224])\n",
            "torch.Size([3, 224, 224])\n",
            "torch.Size([3, 224, 224])\n",
            "torch.Size([3, 224, 224])\n",
            "torch.Size([3, 224, 224])\n",
            "torch.Size([3, 224, 224])\n",
            "torch.Size([3, 224, 224])\n",
            "torch.Size([3, 224, 224])\n",
            "torch.Size([3, 224, 224])\n",
            "torch.Size([3, 224, 224])\n",
            "torch.Size([3, 224, 224])\n",
            "torch.Size([3, 224, 224])\n",
            "torch.Size([3, 224, 224])\n",
            "torch.Size([3, 224, 224])\n",
            "torch.Size([3, 224, 224])\n",
            "torch.Size([3, 224, 224])\n",
            "torch.Size([3, 224, 224])\n",
            "torch.Size([3, 224, 224])\n",
            "torch.Size([3, 224, 224])\n",
            "torch.Size([3, 224, 224])\n",
            "[[0, tensor([0.9024])], [0, tensor([0.9053])], [19, tensor([0.8758])], [0, tensor([0.8804])], [0, tensor([0.9074])], [0, tensor([0.8828])], [0, tensor([0.8564])], [19, tensor([0.8838])], [0, tensor([0.8916])], [0, tensor([0.9016])], [19, tensor([0.8538])], [0, tensor([0.8701])], [0, tensor([0.8818])], [0, tensor([0.8638])], [0, tensor([0.8767])], [0, tensor([0.8904])], [0, tensor([0.9005])], [0, tensor([0.8909])], [0, tensor([0.8807])], [22, tensor([0.8691])]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PzsYPvWjuy_t"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}